# WebCrawler
 Web Crawler
 
crawling/aeromorus.py, dji.py, geobox.py - web-crawler'ы. Чтобы запуститить их, нужно написать scrapy crawl {название фалйа web-crawler'а.py}. Краулеры работают по заданным правилам: Rules. Дальше выдаются url всех этих разрешенных страниц, по которым можно перейти с разрешенного домена: allowed_domains. Краулер выдаст url-адреса всех товаров. Дальше данные этих товаров должны были быть получены, но именно только эта часть не сделана. После этого полученные данные загружаются в excel-файл с помощью excelhandler.py.
excelhandler.py: реализует класс XLWriter для записи данных из словаря в .xlsx файл.
Конструктор класса:
XLWriter(data) - принимает на вход data массив словарей для обработки; создает файл для записи output.xlsx или открывает уже существующий, записывает в него заголовки столбцов, соответствующие ключам словаря, если таких ещё нет в файле; если это запись в уже существующий файл, обновляет закраску ячеек, закрашивая только новые.
populateSheet() - записывает сохраненные в переменной data данные из словаря в таблицу, закрашивает новые ячейки.
save() - сохраняет файл Excel-таблицы.

Пример использования (представлен в файле реализации):
data = [{},...{},] # массив словарей; каждый словарь содержит наименование, категорию и прочие характеристики конкретной позиции с сайта
w = XLWriter(data) # конструктор класса
w.populateSheet() # заполняем открытый в конструкторе файл таблицы данными
w.save() # сохраняем и закрываем таблицу
